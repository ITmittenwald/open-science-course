<section>
<h2>Open Science</h2>
<h3 style="margin-top: 0.5em">
    Vaclav (Vashek) Petras</h3>
<p class="title-foot">
    <a href="http://www.ncsu.edu/" title="North Carolina State University">NCSU</a>
    <a href="http://geospatial.ncsu.edu/osgeorel/" title="NCSU GeoForAll Lab">GeoForAll Lab</a>
    at the
    <a href="http://geospatial.ncsu.edu/" title="Center for Geospatial Analytics">Center for Geospatial Analytics</a>
    <br>
    North Carolina State University
</p>
<p>
    GIS 710: Geospatial Analytics for Grand Challenges<br>
    November 12, 2018
</p>
</section>


<section>
    <h3>Reproducibility of Computational Articles</h3>
    <p>
    Stodden et al. (PNAS, March 13, 2018)<br>
    204 computational articles from Science in 2011–2012
    </p>
    <img class="stretch" style="margin: 0px"; src="img/reproducibility_ratio_from_stodden.png" alt="26% reproducible, 74% irreproducible">
    <small class="credit">
    Stodden, V., Seiler, J., &amp; Ma, Z. (2018).
    An empirical analysis of journal policy effectiveness for computational reproducibility.
    In: <em>Proceedings of the National Academy of Sciences</em>
    115(11), p. 2584-2589.
    <a href="https://doi.org/10.1073/pnas.1708290115">DOI&nbsp;10.1073/pnas.1708290115</a>
    </small>
    <aside class="notes">
        A recently published study showed that 74% of computational
        articles published in Science was not computationally reproducible.
        This brings questions about validity of findings in these
        articles and also about the peer-review process.
        <!--
        Are these articles useful even to read when we don't know how
        the findings were obtained? ...let alone applying the findings.
        -->
    </aside>
</section>

<!--
Stodden et al. (2018):
We were able to obtain data and code from the authors of 89
articles in our sample of 204, giving an estimate for the artifact
recovery rate of 44% for articles published in Science shortly
after the policy change: (65 + 24/204) with a 95% bootstrap
confidence interval of [0.36, 0.50]. Of the 56 articles that were
then deemed potentially reproducible, we randomly chose 22
to attempt replication, and all but 1 of the 22 provided enough
information that we were able to reproduce their computational
findings (given sufficient resources and a willingness write some
code). We estimate 95% (21/22) of the articles deemed reproducible
by inspection are computationally reproducible, so for
the full sample, we estimate 26% will computationally reproduce
((56 ∗ (1 − 1/22))) with a 95% bootstrap confidence interval for
the proportion [0.20, 0.32].
((56 * (1 - 1./22)) / 204)
-->


<section>
<h2>Open Science Beginnings</h2>

<p>
First journal ever published:<br>
<em>Philosophical Transactions (of the Royal Society)</em>
</p>

<img src="img/philosophical_transactions.jpg" class="stretch">
<p class="credit glow">
    CC BY Stefan Janusz,
    <a href="https://en.wikipedia.org/wiki/File:Philosophical_Transactions_Volume_1_frontispiece.jpg">Wikipedia</a>
</p>
</section>


<section>
<h2>Publishing goals</h2>
<ul>
    <li>registration <small>so that scientists get credit</small>
    <li>archiving <small>so that we preserve knowledge for the future</small>
    <li>dissemination <small>so that people can use this knowledge</small>
    <li>peer review <small>so that we know it's worth it</small>
</ul>

<p>
<small>
    Discussion questions:
    Do you agree with these publishing goals?
    How are these different from goals of science?
    Are these publishing goals fulfilled by journal papers?
</small>
</section>


<section>
<h3>Open Science</h3>

<img src="img/spectrum_of_reproducible_research.png" class="stretch">
<small>[Buckheit and Donoho 1995, Peng 2011, Rodríguez-Sánchez et al. 2016, Marwick 2016]</small>
<p class="credit">
    Image credit: CC BY-SA Comtebenoit,
    <a href="https://commons.wikimedia.org/wiki/File:Spectrum_of_reproducible_research.png">Wikimedia</a>
    <a href="https://commons.wikimedia.org/wiki/Commons:Undeletion_requests/Archive/2017-05#File:Reproducibility_Spectrum.png">(note: file freedom disputed)</a>
<!--
Peng, R. D. (2011). Reproducible research in computational science.
Science, 334(6060), 1226-1227.
DOI: 10.1126/science.1213847

Rodríguez-Sánchez, F., Bartomeus, I., Varela, S., Pérez-Luque, A.J.,
Ciencia reproducible: qué, por qué, cómoEcosistemas [en linea] 2016, 25
(Mayo-Agosto) : [Fecha de consulta: 4 de abril de 2017]
Disponible en:<http://www.redalyc.org/articulo.oa?id=54046745011>
ISSN 1132-6344

Marwick, B. (2016). Computational reproducibility in archaeological research:
basic principles and a case study of their implementation.
Journal of Archaeological Method and Theory, 1-27.
DOI: 10.1007/s10816-015-9272-9

There was "significant doubt about the freedom of a particular file"
(https://commons.wikimedia.org/wiki/Commons:Undeletion_requests/Archive/2017-05#File:Reproducibility_Spectrum.png)
but the references (especially with those added here) are present
(covering the line of authors of the idea and the image),
there was an effort and intention from authors to make things CC,
and uncopyrightable concepts were used when creating the image,
so leaving it here as is.
-->
</p>

<p>
<small>
    Discussion questions:
    Are you offended?
    On which side of the spectrum have you published?
</small>

</section>


<section>
<h3>Open Science Components</h3>

<ul class="left">
<li>6 pillars <small>[Watson 2015]</small>:
    <ul>
        <li>open methodology
        <li>open access
        <li>open data
        <li>open source
        <li>open peer review
        <li>open education (or educational resources)
    </ul>
<li>other components:
    <ul>
        <li>open hardware
        <li>open formats
        <li>open standards
    </ul>
</ul>

<ul class="right">
<li>related concepts:
    <ul>
        <li>Open-notebook science
        <li>Science 2.0 (like Web 2.0)
        <li>Citizen science
        <li>Public science
        <li>Participatory research
        <li>Open innovation
        <li>Crowdsourcing
        <li>Preprints
        <li>Inner source
    </ul>
</ul>

<p>
<small>
    Discussion question:
    Is there something you would add to the list?
</small>

</section>

<!--
Watson, M. (2015). When will ‘open science’ become simply ‘science’?.
Genome biology, 16(1), 101. doi:10.1186/s13059-015-0669-2
There are six commonly accepted pillars of open science: open data,
open access, open methodology, open source, open peer review and open education.
-->

<section>
<h3>The “re” words</h3>

No agreement on some of the definitions especially accords different field;
definitions are often overlapping or swapped.

<ul>
    <li>replicability <small>independent validation of specific findings</small>
    <li>repeatability <small>same conditions, people, instruments, ... (test–retest reliability)</small>
    <li>recomputability <small>same results in computational research</small>
    <li>reproducibility <small>obtain again same results from the raw data</small>
    <li>reusability <small>use again the same data or methods</small>
</ul>

<small>
For example, Ince et al. (2012) in computational science
distinguishes direct reproducibility as rerunning of the code and
indirect reproducibility as validate something other than the entire code.

</small>

<p>
<small>
    Discussion questions:
    How do want science to look like?
    Are there some minimal requirements?
    What should be possible or easy to do for you
    when you receive a scientific publication?
</small>

</section>


<section>
<h3>Internal reasons for open science</h3>

Internal or selfish reasons for doing open science

<ul>
    <li>in your lab:
        <ul>
            <li>collaboration <small>work together with your colleagues</small>
            <li>transfer <small>transfer research between you and your colleague</small>
        </ul>
    <li>yourself:
        <ul>
            <li>revisit <small>revisit or return to a project after some time</small>
            <li>correction <small>correct a mistake in the research</small>
            <li>extension <small>improve or build on an existing project</small>
        </ul>
</ul>

<p>
<small>
    Discussion questions:
    What is your experience with getting back to your own research
    or continuing research started by someone else?
</small>

</section>
